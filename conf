###    General variables.

# Project name.
# This will be used for files and folder names.
PROJECT="coti"

# Help message
HELP="
Before running the script, make sure that conf is updated and the steps in
the script itself suits your needs.
./run will start its thing and its expecting access to internal resources.
There is no reason to think it will work outside the internal network.

Parameters:
   -h, --help: Print this message.
   clean:   Cleans the system of _any_ virtual resource.
   host:    Host preparation, making sure the host and resources are ready.
   uc-prep: Prepares the undercloud machine for undercloud installation.
   uc-inst: Install the unercloud and some tweaks.
   uc-post: Runs post installation tasks.
   oc-prep: Overcloud preperations (images, containers, flavors etc').
   oc-depl: Overcloud deployment.
   oc-post: Runs post deployment tasks.
   bnr:     Backup and restore, this will backup the undercloud machine and
            restore onto a new undercloud. It needs a running deployment.
   test:    Run tests from the tests folder on a deployment.
   full:    Aims to deploy a working environment.

For a full deployment:
    ./run clean
    ./run host
    ./run uc-prep
    ./run uc-inst
    ./run uc-post
    ./run oc-prep
    ./run oc-depl
    ./run oc-post
    ./run test
Or  ./run full
"


# The script works here.
# It is deleted and recreated in each run.
WORK_DIR="/tmp/$PROJECT"

# Starting location, used to resolve paths.
CWD=$(pwd)

# Shortcut to the tripleo heat templates.
THT="/usr/share/openstack-tripleo-heat-templates"

# Shortcut to libvirt's image folder.
VIRT_IMG="/var/lib/libvirt/images"

# Log file
BASE_LOG_FILE="$(pwd)/logs/$PROJECT-$(date +%H%M%S)"

# Selinux state on the undercloud.
UNDER_SEL="permissive"

# Selinux state on the overcloud.
OVER_SEL="permissive"

# Undercloud admin password
ADMIN_PASSWORD="12345678"

# How images are accessed with virt-commands.
LIBGUESTFS_BACKEND="direct"

# Base path to EPEL
EPEL="https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages"

# Messge to display on long operations
LONG="(This step takes time.)"

# RPM location of latest rhos-release
LATEST_RR="http://rhos-release.virt.bos.redhat.com/repos/rhos-release/rhos-release-latest.noarch.rpm"

# TODO:
# I need to fix this part to search for specific puddles.
# Version to install deploy.
OS_VER=14
PUDDLE_VER="latest"

# Argument to pass to rhos-release.
RR_CMD="$OS_VER"

# Do we obtain images from predefined images or do we create our own.
# prepare_puddle_images - create our own.
# edit_predefined_images - edit images from RPM.
# predefined_images - use untouched images from RPM.
OBTAIN_IMAGES="predefined_images"

# Do images and proto gets updated to the latest bits from the repos.
UPDATE_IMAGE=true

# Is the Overcloud deployeds via UI? If so you want to stop this script after
# the overcloud pre-deploy stage and skip introspection.
VIA_UI=false

# DNS to be used.
# The default here is to use the host's configuration.
DNS=$(grep nameserver /etc/resolv.conf | head -n 1 | cut -d " " -f 2)

# Host default nic.
NIC=$(ip route get $DNS | grep dev | awk '{print $5}')

# Host IP.
HOST_IP=$(ifconfig $NIC | grep "inet " | awk '{print $2}')

# Host root password.
HOST_PASS="12345678"

# virt-customize arguments.
VIRSH_CUST="-m 8192 --smp 6 -q"

# SSH arguments.
SSH_CUST="/usr/bin/ssh -qtt -o StrictHostKeyChecking=no"

# Package management arguments.
PKG_CUST="yum -q -y"

# Installing Telemetry, its false by default.
USE_TELEMETRY=true

# RHOS docker registry
RHOS_REG="docker-registry.engineering.redhat.com"

# CEPH docker registry
CEPH_REG="brew-pulp-docker01.web.prod.ext.phx2.redhat.com:8888"

# Letters array
LETTERS=( "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s" "t" "u" "v" "w" "x" "y" "z" )

###   File server location.   
###   This is were the script gets stuff like guest images,
###   and also where it keeps modified ones.

# Server to download images from.
FILE_SERVER="http://ikook.tlv.redhat.com"

# Server's domain.
SRV_DOMAIN=${FILE_SERVER##*://}

# RHEL guest image.
RHEL_VERSION="7.6"
RHEL_GUEST="$FILE_SERVER/gen_images/cloud/rhel-guest-image-${RHEL_VERSION}-latest.x86_64.qcow2"
GUEST_FILE=$(basename $RHEL_GUEST)

# Director image RPMs path.
OC_IMAGES="$FILE_SERVER/rpms/rhos$OS_VER/rhosp-director-images-$OS_VER-latest.rpm"
OC_IPA="$FILE_SERVER/rpms/rhos$OS_VER/rhosp-director-images-$OS_VER-ipa-latest.rpm"

# Path to the folder that contains all the repos
AUTO_PATH="$FILE_SERVER/auto"

# Guest image time zone.
# The default is taken from the host's.
GUEST_TZ=$(timedatectl | grep "Time zone" | awk '{print $3}')

# How and where to upload images.
UPLOAD_URL="${SRV_DOMAIN}:/home/ftp/auto"
UPLOAD_DIR=${UPLOAD_URL##*:}
UPLOAD_USER="rhos-qe"
UPLOAD_PASS="qum5net"

# Nodes and undercloud's root password.
ROOT_PASS="12345678"

# NTP server to be used.
# Default is taken from the host.
NTP=$(grep "'^server'\|iburst" /etc/ntp.conf | cut -d " " -f 2 | head -n 1)

# Backup and restore. Starting Liberty, a backing up and restore mechanism for the
# Undercloud node was created. If set to true here, undercloud-0 will be backed-up,
# destroyed and undercloud-1 will be installed with all the settings and data.

BCK_RES=true

###   Nodes and per node settings.

# Node names.
# Nodes will be _defined_ in this order.
# Keep undercloud first!
NODES=(
        "undercloud" 
        "controller"
        "compute"
        "ceph"
      )

# Network setup per node.
# Always keep provisioning network first and the external network last.
NETWORKS=(
           "ControlPlane"
           "StorageIP"
           "StorageMGT"
           "Internal"
           "Tenant"
           "External"
         )

# Undercloud's local interface and host adresses.
LOCAL_INTERFACE="eth0"
CIDR="192.168.24"
LOCAL_IP="${CIDR}.1/24"
PUBLIC_HOST="${CIDR}.2"
ADMIN_HOST="${CIDR}.3"

# External network IP range. This is needed to attach an IP for accessing the
# nodes externally bypassing some limitations caused from network isolation.
# There is a pair of ranges, one for internal access and one for external.
# The number here represent the final (rightmost) notation.
# '1' is reserved for the gateway.
# Introspection IPs are in the same CIDR but have a differnt range.
DHCP_IN_START=5
DHCP_IN_END=90
DHCP_INTRO_START=91
DHCP_INTRO_END=120
DHCP_OUT_START=121
DHCP_OUT_END=250

# Node numbers and settings.
# !! The base name needs to be the same as the node names in the NODES array.
# NAM - Name of the node.
# FLV - Flavor to be used for nodes.
# NUM - How many of that node.
# RAM - RAM in MB.
# SWP - SWAP in MB.
# CPU - How many vCPU.
# DUM - Extra (Dummy) nodes of the same type.
# DSK - Size of disk in GB. Nodes can work with 12, Undercloud needs 30. 
# OSD - Ceph specific, this is the size for the Ceph OSD.
# SDX - How many disks (SDx's) per node. Ceph needs 6: 1 for root and 5 OSDs.

# undercloud nodes
undercloud_NAM="undercloud"
undercloud_NUM=1
undercloud_RAM=24576
undercloud_SWP=256
undercloud_CPU=6
undercloud_DUM=0
undercloud_DSK=32
undercloud_SDX=1

# controller nodes
controller_NAM="control"
controller_FLV="control"
controller_NUM=3
controller_RAM=24576
controller_SWP=256
controller_CPU=4
controller_DUM=0
controller_DSK=22
controller_SDX=1

# compute nodes
compute_NAM="compute"
compute_FLV="compute"
compute_NUM=3
compute_RAM=24576
compute_SWP=256
compute_CPU=4
compute_DUM=0
compute_DSK=22
compute_SDX=1

# ceph nodes
ceph_NAM="ceph"
ceph_FLV="ceph-storage"
ceph_NUM=1
ceph_RAM=8192
ceph_SWP=256
ceph_CPU=6
ceph_DUM=0
ceph_DSK=12
ceph_SDX=6

TOT_NODES=$(( controller_NUM + compute_NUM + ceph_NUM ))
